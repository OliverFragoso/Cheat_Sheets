Write a simple unit test using pytest
  1) Nombre "test_" el archivo a probar, Pytest al ver "test_" entiende que es un file a ser evaluado
    - test_prueba
  2) importar los test modules
    - import pytest
      from <nombre del file>(sin el .py) import <funcion>
  3) escribir las "unit test" o nombre de la prueba que se le hará
    - def test_.....():
        assert ... --> todas la pruebas deben tene el assert
        assert function(argumentos) == lo que debe regresar
        assert function(argumentos) is None --> cuando debería regresar nada
  4) Probar el unit test
    - abrir la terminal y escribir: pytest test_... .py
                                    !pytest test_... .py -->en IPuthon shell puede que lleve el signo
                                                            de exclamación
  - Example 1:
    # Import the pytest package
      import pytest

    # Import the function convert_to_int()
      from preprocessing_helpers import convert_to_int

    # Complete the unit test name by adding a prefix
      def test_on_string_with_one_comma():
    # Complete the assert statement
      assert convert_to_int("2,081")==2081
      
Understanding The output
- . El puntito significa que sí pasó el test
- F significa que falló
  - '>' line that raised the exception
  - 'E' contains details of the exception
    - '+ where' dice lo que regresó cuando se corrió la función de prueba
    
ASSERT STATEMENTS
  - Si el valor cumple la condición no se regresa nada
  - Si no se cumple la condición se regresa un mensaje de error
  - Se puede personalizar el mensaje de error
    Example: assert 1 == 2, "One is not equal two"
  
  - Tener cuidado a la hora de comparar doubles
    Example: assert 0.1+0.1+0.1 == 0.3 --> regresarà false
    - Mejor para comparar doubles usar pytest.approx()
      Example: assert 0.1+0.1+0.1 == pytest.aprox(0.3)
    - pytest tambien funciona con arrays
      np.array([0.1+0.1+0.1] == pytest.approx(np.array([0.3]))
      
Write an informative test failure message
  - def test_on_string_with_one_comma():
      test_argument = "2,081"
      expected = 2081
      actual = convert_to_int(test_argument)
    # Format the string with the actual return value
      message = "convert_to_int('2,081') should return the int 2081, but it actually returned {0}".format(actual)
    # Write the assert statement which prints message on failure
      assert actual == expected, message
      
  - def test_on_clean_file():
      expected = np.array([[2081.0, 314942.0], [1059.0, 186606.0],[1148.0, 206186.0]])
      actual = get_data_as_numpy_array("example_clean_data.txt", num_columns=2)
      message = "Expected return value: {0}, Actual return value: {1}".format(expected, actual)
      # Complete the assert statement
      assert actual == pytest.approx(expected), message
      
Context Manager with Pytest
  - Tiene como finalidad silenciar si la prueba levanta algun error que le indiquemos como ValueError
  - Syntaxis
    # Fill in with a context manager that will silence the ValueError
      with pytest.raises(ValueError):
        raise ValueError
  - Example:
    import pytest

    try:
    # Fill in with a context manager that raises Failed if no OSError is raised
      with pytest.raises(OSError):
        raise ValueError
    except:
      print("pytest raised an exception because no OSError was raised in the context.")
  
  - Example: 
    # Store the raised ValueError in the variable exc_info
    with pytest.raises(ValueError) as  exc_info:
      raise ValueError("Silence me!")
      
  - Example:
    import pytest

    with pytest.raises(ValueError) as exc_info:
      raise ValueError("Silence me!")
    # Check if the raised ValueError contains the correct message
    assert exc_info.match("Silence me!")
    
  - Example: se verifica que la función regrese el mensaje de error 
    def test_on_one_row():
      test_argument = np.array([[1382.0, 390167.0]])
    # Store information about raised ValueError in exc_info
      with pytest.raises(ValueError) as exc_info:
        split_into_training_and_testing_sets(test_argument)
      expected_error_msg = "Argument data_array must have at least 2 rows, it actually has just 1"
    # Check if the raised ValueError contains the correct message
      assert exc_info.match(expected_error_msg)
      
THE WELL TESTED ARGUMENTS
  - Se deben probar en 3 categorias
    1) Bad arguments: cuando el código regresa un exception como: ValueError
    2) Especial arguments
      2.1) Boundary values
      2.2) Valores especiales o se necesite una lógica en específico para calcular la respuesta 
    3) Normal arguments
  - No siempre se cuenta con los casos de Bad arguments o Especial arguments así que si sólo se prueba
    con un criterio tambièn està bien
    
TEST DRIVE DEVELOPMENT (TDD)
  - Trata de asegurarse de las unidades de prueba queden escritas y no olvides programarlas
  - Se declaran los tests antes de programar las funciones porque esto permite que el desarrollo
    sea más claro y se contemplen los posibles casos de fallo
    
  - Se hace la declaración de los Tests, si se llegan a correr mandan un error porque la función todavía 
    no se crea
    Example:
    # Give a name to the test for an argument with missing comma
      def test_on_string_with_missing_comma():
        actual = convert_to_int("178100,301")
        assert actual is None, "Expected: None, Actual: {0}".format(actual)
    
    def test_on_string_with_incorrectly_placed_comma():
    # Assign to the actual return value for the argument "12,72,891"
        actual = convert_to_int("12,72,891")
        assert actual is None, "Expected: None, Actual: {0}".format(actual)
    
    def test_on_float_valued_string():
        actual = convert_to_int("23,816.92")
    # Complete the assert statement
        assert actual is None, "Expected: None, Actual: {0}".format(actual)

  - Se puede poner un try catch a la hora de regresar el valor
    Example:
    # Give a name to the test for an argument with missing comma
    def test_on_string_with_missing_comma():
        actual = convert_to_int("178100,301")
        assert actual is None, "Expected: None, Actual: {0}".format(actual)
    
    def test_on_string_with_incorrectly_placed_comma():
    # Assign to the actual return value for the argument "12,72,891"
        actual = convert_to_int("12,72,891")
        assert actual is None, "Expected: None, Actual: {0}".format(actual)
    
    def test_on_float_valued_string():
        actual = convert_to_int("23,816.92")
    # Complete the assert statement
        assert actual is None, "Expected: None, Actual: {0}".format(actual)

Cómo organizar todos los Tests
Test Class
  - Syntaxis:
    import pytest
    from ... import..
    
    class TestNameClass(object): --> debe tenerla palabra "Test" al inicio y junto
                              al nombre de la función
      def test_nombre1(self): --> dentro de la clase van las distintas pruebas
      def test_nombre2(self):
      
RUNNING TESTS: En Ipython se le agrega un !, 
               por ejemplo: !pytest
  - En la terminal te situas en la carpeta que tenga el script con tus tests y escribes:
    pytest -x --> "-x" indica que se detenga en cuento un test falle
  - correr un pequeño grupo de tests
    pytest <carpeta>/test_module.py
  - Ser más específico y correr una clase en particular
    pytest <carpeta>/test_module.py::<test class name>
  - Todavía más específico, correr cierta test de una clase
    pytest <carpeta>/test_module.py::<test class>::<unit test>
  - Otra forma de correr test es usando keyword expressions (son como wildcards) y se
    denotan por el uso de "-k"
    pytest -k "pattern"
    
    Ejemplo: 
    - pytest -k "<Test class name>" --> corre la clase que contenga ese nombre
    - pytest -k "<part of a class name>" --> se escribe la mitad del nombre completo
                                             de la clase y corre con eso, mientras sea
                                             único el nombre de esa clase
    - pytest -k "TestName and not test_module" --> corre los tests de esta clase excepto
                                                   el unit test especificado
                                                   
EXPECTED FAILURES AND CONDITIONAL SKIPPING 
  - Cuando se implementa primero los tests antes de crear las funciones del código (TDD)
    se debe avisar al código que estas fallas ya se esperan y eso se hace con el decorator
    "xfail"
    
    import test
    class TestName(object):
      @pytest.mark.xfail(reason = "Using TDD, is not implemented yet")
      def test_unite_name(self):
  - El decorador también se puede poner arriba de la clase
  
  - Avisar que el programa puede fallar bajo ciertas condiciones y no queremos que nos avise
    - Versiones de Python
    - Plataformas: Windows, Mac, Linux
    
    class TestName(object):
    @pytest.mark.skipif(boolean_expression)
    def testname(self):
    
    - Ejemplo: se necesita python 2.7 para correr esta versión de código para eso se necesita
               sys
      import sys
      class TestName(object):
        @pytest.mark.skipif(sys.version_info > (2,7),reason = "requires Python 2.7")
        def testname(self):
      
      sys.platform --> Se utiliza para checar plataformas: Windows, Mac o Linux 
    
    - pytest -r[set of characters]: Mostrar la razón de la falla en el reporte
      - pytest -rs: tests que fueron skipeados cerca del final
      - pytest -rx: tests que son xfailed con su razón 
      - pytest -rxs: se pueden combinar 
     
CONTINUOUS INTEGRATION AND CODE COVERAGE
  - Se usará en este caso Travis CI como servidor
    1) Crear un settings file llamado ".travis.yml" en el root de nuestro repositorio
      - Contenido (tiene 2 secciones):
        language: python 
        python:
          - "3.6" #Ponemos la versión con la que estemos trabajando
        install:
          - pip install -e
        script
          - pytest tests
    2) hacer push del archivo a GitHub
      - git add .travis.yml
      - git push origin master 
    3) Instalar aplicación de Travis en GitHub
      1) en la página principal ir a "Marketplace" 
      2) buscar "Travis"
      3) instalar la app
      4) dar acceso a los repositorios y grupos correspondientes
    4) Mostrar el status 
      1) dar cick en "build passing"
      2) seleccionar "Markdown" en el dropdown
      3) pegar el código de markdown en el README file
  - Code coverage
    - Indica el % del código de la aplicación que se corre cuando se corre el test suite.
      Entre mayor sea el % mejor
    
